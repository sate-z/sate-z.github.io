<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Python_Scrapy入门 | SateZheng</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/5.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Python_Scrapy入门</h1><a id="logo" href="/.">SateZheng</a><p class="description">多读书，多看报，少吃零食，多睡觉</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Python_Scrapy入门</h1><div class="post-meta">Dec 15, 2016<span> | </span><span class="category"><a href="/categories/Python爬虫/">Python爬虫</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#安装"><span class="toc-number">1.</span> <span class="toc-text">安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#创建项目："><span class="toc-number">2.</span> <span class="toc-text">创建项目：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编写第一个爬虫"><span class="toc-number">3.</span> <span class="toc-text">编写第一个爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编写第二个项目（从选定的url地址中提取我们想要的信息）"><span class="toc-number">4.</span> <span class="toc-text">编写第二个项目（从选定的url地址中提取我们想要的信息）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#追踪链接项目"><span class="toc-number">5.</span> <span class="toc-text">追踪链接项目</span></a></li></ol></div></div><div class="post-content"><blockquote>
<p>参考官方文档：<a href="http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/tutorial.html#spider" target="_blank" rel="external">http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/tutorial.html#spider</a></p>
</blockquote>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>安装分别在<code>ubunut</code>和<code>MAC</code>系统下安装，过程如下：</p>
<p><code>ubuntu</code>系统：</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 使用pip安装Scrapy</span></div><div class="line">sudo pip <span class="keyword">install </span><span class="keyword">Scrapy</span></div><div class="line"><span class="comment"># 遇到报错，报哪个包没有，就是用pip安装哪个</span></div></pre></td></tr></table></figure>
<p><code>MAC</code>系统:</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 先确保已经安装 xcode</span></div><div class="line">xcode-select --<span class="keyword">install</span></div><div class="line"><span class="comment"># 使用 pip 安装 Scrapy</span></div><div class="line">sudo pip <span class="keyword">install </span><span class="keyword">Scrapy</span></div><div class="line"></div><div class="line"><span class="comment"># 遇到报错 关于six-1.4.1的，无法升级到新的版本。如下两种解决办法：</span></div><div class="line"><span class="number">1</span>、 sudo pip <span class="keyword">install </span><span class="keyword">scrapy </span>--ignore-<span class="keyword">installed </span>six <span class="comment"># 跳过</span></div><div class="line"><span class="number">2</span>、 sudo easy_install <span class="string">"six&gt;=1.5.2"</span> <span class="comment"># 使用easy_install 升级six。然后安装</span></div></pre></td></tr></table></figure>
<h3 id="创建项目："><a href="#创建项目：" class="headerlink" title="创建项目："></a>创建项目：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">$ scrapy startproject tutorial  # 创建项目</div><div class="line">$ cd tutorial/ ; tree		    # 进入到目录，并展示目录结构</div><div class="line">.</div><div class="line">├── scrapy.cfg</div><div class="line">└── tutorial</div><div class="line">    ├── __init__.py</div><div class="line">    ├── items.py				# 保存爬去到的数据的容器,继承 scrapy.Item 类</div><div class="line">    ├── pipelines.py</div><div class="line">    ├── settings.py</div><div class="line">    └── spiders</div><div class="line">        └── __init__.py</div><div class="line">        └── dmoz_spider.py  	 # 该文件为自己创建，继承 scrapy.Spider 类。定义属性：</div><div class="line">        						 # name（唯一，区别spider）</div><div class="line">        						 # start_urls（spider启动时进行爬去的列表，第一个被获取到</div><div class="line">        						 的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提</div><div class="line">        						 取）</div><div class="line">        						 # pasrse() 方法，被调用时，每个初始URL完成下载后生成的</div><div class="line">        						 Response 对象将会作为唯一的参数传递给该函数。 该方法负责解</div><div class="line">        						 析返回的数据(response data)，提取数据(生成item)以及生成</div><div class="line">        						 需要进一步处理的URL的 Request 对象。</div></pre></td></tr></table></figure>
<h3 id="编写第一个爬虫"><a href="#编写第一个爬虫" class="headerlink" title="编写第一个爬虫"></a>编写第一个爬虫</h3><p>目的：获取url页面源码。(并未用到上边定义的<code>Items</code>)</p>
<p>创建一个<code>spider</code>，继承<code>scrapy.Spider</code>类，并定义一些属性：</p>
<ul>
<li><code>name:</code> 用于区别<code>Spider</code>。 该名字必须是唯一的，不可以为不同的<code>Spider</code>设定相同的名字。</li>
<li><code>start_urls:</code> 包含了<code>Spider</code>在启动时进行爬取的<code>url</code>列表。 因此，第一个被获取到的页面将是其中之一。 后续的<code>URL</code>则从初始的<code>URL</code>获取到的数据中提取</li>
<li><code>parse():</code> 是<code>spider</code>的一个方法。 被调用时，每个初始<code>URL</code>完成下载后生成的<code>Response(页面内容)</code>对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成<code>item</code>)以及生成需要进一步处理的<code>URL</code>的<code>Request</code>对象。</li>
</ul>
<p>在<code>tutorial/spiders</code>目录中创建<code>dmoz_spider.py</code>，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding=utf-8</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"dmoz"</span></div><div class="line">    allow_domains = [<span class="string">"dmoz.org"</span>]</div><div class="line">    start_urls = [</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span></div><div class="line">    ]</div><div class="line"></div><div class="line"><span class="comment">#----- 从start_urls中读取页面源码信息,并写入本地---#</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self,response)</span>:</span></div><div class="line">        <span class="comment"># reponse.body 会输出请求url的源码,response.url 是所请求的 url 地址</span></div><div class="line">        <span class="comment"># 通过下面的输出语句发现，start_urls 中的url地址的请求结果被分别带入到该方法中。</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"debug---------"</span></div><div class="line">        <span class="keyword">print</span> response.body</div><div class="line">        <span class="keyword">print</span> response.url</div><div class="line">        <span class="keyword">print</span> <span class="string">"debug----------"</span></div><div class="line"></div><div class="line">        <span class="comment"># 过滤出请求 url 地址的最后一段，并以该段的名字来创建文件，并写入对应的网页源码。</span></div><div class="line">        filename = response.url.split(<span class="string">"/"</span>)[<span class="number">-2</span>] + <span class="string">'.html'</span></div><div class="line">        <span class="keyword">with</span> open(filename,<span class="string">"wb"</span>) <span class="keyword">as</span> f:</div><div class="line">            f.write(response.body)</div></pre></td></tr></table></figure>
<p><strong>执行：</strong></p>
<p>进入项目的根目录，执行下列命令启动spider</p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$ </span>scrapy crawl dmoz</div></pre></td></tr></table></figure>
<p>执行结果：在项目目录下，会生成两个文件，<code>Books.html</code>和<code>Resources.html</code>，文件内容分别是两个url页面的源码。</p>
<h3 id="编写第二个项目（从选定的url地址中提取我们想要的信息）"><a href="#编写第二个项目（从选定的url地址中提取我们想要的信息）" class="headerlink" title="编写第二个项目（从选定的url地址中提取我们想要的信息）"></a>编写第二个项目（从选定的url地址中提取我们想要的信息）</h3><p><strong>定义Item</strong></p>
<blockquote>
<p><code>Item</code>是保存爬取到的数据的容器。其使用方法和字典类似，虽然可以在<code>Scrapy</code>中直接使用<code>dict</code>，但是<code>Item</code>提供了额外保护机制来避免拼写错误导致的未定义字段错误。</p>
</blockquote>
<p>编辑<code>tutorial</code>目录中的<code>items.py</code>文件,根据我们需要获取到的数据对<code>item</code>进行建模。下边分别定义了<code>title</code>、<code>url</code>和网站的描述。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="comment"># Define here the models for your scraped items</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># See documentation in:</span></div><div class="line"><span class="comment"># http://doc.scrapy.org/en/latest/topics/items.html</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TutorialItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    <span class="comment"># define the fields for your item here like:</span></div><div class="line">    <span class="comment"># name = scrapy.Field()</span></div><div class="line">    title = scrapy.Field()  </div><div class="line">    link = scrapy.Field()</div><div class="line">    desc = scrapy.Field()</div></pre></td></tr></table></figure>
<p>通过开发者工具对页面的源码进行分析，我们要提取的信息如下：<br><img src="http://7xkm8w.com1.z0.glb.clouddn.com/scrapy_dmoz.png" alt="源码分析"></p>
<p>在<code>tutorial/spiders</code>目录中创建<code>dmoz_spider.py</code>，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding=utf-8</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> DmozItem</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"dmoz"</span></div><div class="line">    allow_domains = [<span class="string">"dmoz.org"</span>]</div><div class="line">    start_urls = [</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span></div><div class="line">    ]</div><div class="line"></div><div class="line"><span class="comment"># ----- 从start_urls中的页面源码中提取自己需要的,title、link、简介</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line"></div><div class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">'//*[@class="title-and-desc"]'</span>):</div><div class="line">        <span class="comment"># item 对象是自定义的 python 字典，可以使用标准的字典语法来获取到每个段子的值，字段就是之前在items.py文件中用Field赋值的属性。</span></div><div class="line">            item = DmozItem()</div><div class="line">            item[<span class="string">'title'</span>] = sel.xpath(<span class="string">'a/div/text()'</span>).extract()</div><div class="line">            item[<span class="string">'link'</span>] = sel.xpath(<span class="string">'a/@href'</span>).extract()</div><div class="line">            item[<span class="string">'desc'</span>] = sel.xpath(<span class="string">'div/text()'</span>).extract()</div><div class="line">            <span class="keyword">yield</span> item</div></pre></td></tr></table></figure>
<p><strong>执行：</strong></p>
<p>进入项目的根目录，执行下列命令启动spider</p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$ </span>scrapy crawl dmoz</div></pre></td></tr></table></figure>
<p>在输出的<code>debug</code>信息中，可以看到生成的<code>items</code>列表。更直观一点可以将<code>items</code>写入文件：</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$  <span class="keyword">scrapy </span>crawl dmoz -o items.<span class="keyword">json </span>-t <span class="keyword">josn</span></div><div class="line"><span class="comment"># -o 指定文件名称  -t 指定格式</span></div></pre></td></tr></table></figure>
<p>查看<code>items.json</code>内容：</p>
<figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ cat items.json | head -n 5</div><div class="line">[</div><div class="line">&#123;"title": ["eff-bot's Daily Python URL "], "link": ["http://www.pythonware.com/daily/"], "desc": ["<span class="symbol">\r</span><span class="symbol">\n</span><span class="symbol">\t</span><span class="symbol">\t</span><span class="symbol">\t</span><span class="symbol">\r</span><span class="symbol">\n</span>                                    Contains links to assorted resources from the Python universe, compiled by PythonWare.<span class="symbol">\r</span><span class="symbol">\n</span>                                    ", "<span class="symbol">\r</span><span class="symbol">\n</span>                                  "]&#125;,</div><div class="line">&#123;"title": ["O'Reilly Python Center "], "link": ["http://oreilly.com/python/"], "desc": ["<span class="symbol">\r</span><span class="symbol">\n</span><span class="symbol">\t</span><span class="symbol">\t</span><span class="symbol">\t</span><span class="symbol">\r</span><span class="symbol">\n</span>                                    Features Python books, resources, news and articles.<span class="symbol">\r</span><span class="symbol">\n</span>                                    ", "<span class="symbol">\r</span><span class="symbol">\n</span>                                  "]&#125;,</div><div class="line">&#123;"title": ["Python Developer's Guide "], "link": ["https://www.python.org/dev/"], "desc": ["<span class="symbol">\r</span><span class="symbol">\n</span><span class="symbol">\t</span><span class="symbol">\t</span><span class="symbol">\t</span><span class="symbol">\r</span><span class="symbol">\n</span>                                    Resources for reporting bugs, accessing the Python source tree with CVS and taking part in the development of Python.<span class="symbol">\r</span><span class="symbol">\n</span>                                    ", "<span class="symbol">\r</span><span class="symbol">\n</span>                                  "]&#125;,</div><div class="line">&#123;"title": ["Social Bug "], "link": ["http://win32com.goermezer.de/"], "desc": ["<span class="symbol">\r</span><span class="symbol">\n</span><span class="symbol">\t</span><span class="symbol">\t</span><span class="symbol">\t</span><span class="symbol">\r</span><span class="symbol">\n</span>                                    Scripts, examples and news about Python programming for the Windows platform.<span class="symbol">\r</span><span class="symbol">\n</span>                                    ", "<span class="symbol">\r</span><span class="symbol">\n</span>                                  "]&#125;</div></pre></td></tr></table></figure>
<h3 id="追踪链接项目"><a href="#追踪链接项目" class="headerlink" title="追踪链接项目"></a>追踪链接项目</h3><p>上边两个项目，<code>url</code>地址都是直接给出，现在需要将一个页面中的<code>url</code>地址提取出来，并依次进行处理。</p>
<p>取<code>http://www.dmoz.org/Computers/Programming/Languages/Python/</code>中<code>Related categories</code>部分的<code>url</code>地址。如图：</p>
<p><img src="http://7xkm8w.com1.z0.glb.clouddn.com/scrapy_dmoz2.png" alt="网站源码"></p>
<p>更改<code>tutorial/items.py</code>文件，加入<code>fromurl</code>，来表明这个信息来自哪个链接。如下：</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="comment"># Define here the models for your scraped items</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># See documentation in:</span></div><div class="line"><span class="comment"># http://doc.scrapy.org/en/latest/topics/items.html</span></div><div class="line"></div><div class="line">import <span class="keyword">scrapy</span></div><div class="line"></div><div class="line">class DmozItem(<span class="keyword">scrapy.Item):</span></div><div class="line">    <span class="comment"># define the fields for your item here like:</span></div><div class="line">    <span class="comment"># name = scrapy.Field()</span></div><div class="line">    title = <span class="keyword">scrapy.Field()</span></div><div class="line">    link = <span class="keyword">scrapy.Field()</span></div><div class="line">    desc = <span class="keyword">scrapy.Field()</span></div><div class="line">    fromurl = <span class="keyword">scrapy.Field()</span></div></pre></td></tr></table></figure>
<p>在<code>tutorial/spiders</code>目录中创建<code>dmoz_spider.py</code>，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding=utf-8</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> DmozItem</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"dmoz"</span></div><div class="line">    allow_domains = [<span class="string">"dmoz.org"</span>]</div><div class="line">    start_urls = [</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/"</span></div><div class="line">    ]</div><div class="line"></div><div class="line"><span class="comment"># ----- 追踪链接----#</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line"><span class="comment"># 提取需要爬取的链接，产生(yield)一个请求， 该请求使用 parse_dir_contents() 方法作为回调函数, 用于最终产生我们想要的数据.。</span></div><div class="line">        <span class="keyword">print</span> response.url</div><div class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="see-also-row"]/a/@href'</span>):</div><div class="line">            url = response.urljoin(link.extract())</div><div class="line">            <span class="keyword">yield</span> scrapy.Request(url,callback=self.parse_dir_contents)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_dir_contents</span><span class="params">(self,response)</span>:</span></div><div class="line"><span class="comment"># 提取信息，放入item中。这边增加了一个fromurl，所以在items.py 文件中，也要加入。</span></div><div class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">'//*[@class="title-and-desc"]'</span>):</div><div class="line">            item = DmozItem()</div><div class="line">            item[<span class="string">'title'</span>] = sel.xpath(<span class="string">'a/div/text()'</span>).extract()</div><div class="line">            item[<span class="string">'link'</span>] = sel.xpath(<span class="string">'a/@href'</span>).extract()</div><div class="line">            item[<span class="string">'fromurl'</span>] = response.url</div><div class="line">            item[<span class="string">'desc'</span>] = sel.xpath(<span class="string">'div/text()'</span>).extract()</div><div class="line">            <span class="keyword">yield</span>  item</div></pre></td></tr></table></figure>
<p><strong>执行：</strong></p>
<p>进入项目的根目录，执行下列命令启动spider</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$  <span class="keyword">scrapy </span>crawl dmoz -o items1.<span class="keyword">json </span>-t <span class="keyword">josn</span></div></pre></td></tr></table></figure>
<p>查看<code>items1.json</code>内容：</p>
<figure class="highlight prolog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">cat items2.json | head -n <span class="number">5</span></div><div class="line">[</div><div class="line">&#123;<span class="string">"link"</span>: [<span class="string">"http://en.wikipedia.org/wiki/Bytecode"</span>], <span class="string">"title"</span>: [<span class="string">"Bytecode "</span>], <span class="string">"fromurl"</span>: <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Interpreted/Bytecode/"</span>, <span class="string">"desc"</span>: [<span class="string">"\r\n\t\t\t\r\n                                    Growing article, with links to many related topics. [Wikipedia, open content, GNU FDL]\r\n                                    "</span>, <span class="string">"\r\n                                  "</span>]&#125;,</div><div class="line">&#123;<span class="string">"link"</span>: [<span class="string">"http://www.parrotcode.org/"</span>], <span class="string">"title"</span>: [<span class="string">"Parrotcode "</span>], <span class="string">"fromurl"</span>: <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Interpreted/Bytecode/"</span>, <span class="string">"desc"</span>: [<span class="string">"\r\n\t\t\t\r\n                                    Home of Parrot Virtual Machine, made for dynamic languages, originally a target for Perl 6 compiler, hosts many language implementations in varied stages of completion: Tcl, Javascript, Ruby, Lua, Scheme, PHP, Python, Perl 6, APL, .NET. Open source.\r\n                                    "</span>, <span class="string">"\r\n                                  "</span>]&#125;,</div><div class="line">&#123;<span class="string">"link"</span>: [<span class="string">"http://vvm.lip6.fr/"</span>], <span class="string">"title"</span>: [<span class="string">"Virtual Virtual Machine "</span>], <span class="string">"fromurl"</span>: <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Interpreted/Bytecode/"</span>, <span class="string">"desc"</span>: [<span class="string">"\r\n\t\t\t\r\n                                    VVM overview, history, members, projects, realizations, publications.\r\n                                    "</span>, <span class="string">"\r\n                                  "</span>]&#125;,</div><div class="line">&#123;<span class="string">"link"</span>: [<span class="string">"http://os.inf.tu-dresden.de/L4/l3elan.html"</span>], <span class="string">"title"</span>: [<span class="string">"ELAN "</span>], <span class="string">"fromurl"</span>: <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Multiparadigm/"</span>, <span class="string">"desc"</span>: [<span class="string">"\r\n\t\t\t\r\n                                    Created 1974 by Technical University of Berlin group, as alternative to BASIC in teaching, for systematic programming, and related styles: top-down, bottom-up, recursive, modular, syntax-directed. Descriptions, brief resource list, documents. English, Deutsch.\r\n                                    "</span>, <span class="string">"\r\n                                  "</span>]&#125;,</div></pre></td></tr></table></figure>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://yoursite.com/2016/12/15/Python-Scrapy入门/" data-id="ciwrx74ef002fuo402riolp8t" class="article-share-link">分享到</a><div class="tags"><a href="/tags/PythonScrapy/">PythonScrapy</a></div><div class="post-nav"><a href="/2016/12/15/Python-Scrapy命令行工具/" class="next">Python_Scrapy命令行工具</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Ansible/">Ansible</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ELK/">ELK</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Gitlab/">Gitlab</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux命令/">Linux命令</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux工具/">Linux工具</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux系统/">Linux系统</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Nginx/">Nginx</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">39</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python爬虫/">Python爬虫</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/SVN/">SVN</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Zabbix/">Zabbix</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Nginx/" style="font-size: 15px;">Nginx</a> <a href="/tags/Ansible/" style="font-size: 15px;">Ansible</a> <a href="/tags/Anisble/" style="font-size: 15px;">Anisble</a> <a href="/tags/ELK/" style="font-size: 15px;">ELK</a> <a href="/tags/Gitlab/" style="font-size: 15px;">Gitlab</a> <a href="/tags/Git/" style="font-size: 15px;">Git</a> <a href="/tags/Gitlab，Git/" style="font-size: 15px;">Gitlab，Git</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/inode/" style="font-size: 15px;">inode</a> <a href="/tags/Logrotate/" style="font-size: 15px;">Logrotate</a> <a href="/tags/Log/" style="font-size: 15px;">Log</a> <a href="/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/tags/VPN/" style="font-size: 15px;">VPN</a> <a href="/tags/Python模块/" style="font-size: 15px;">Python模块</a> <a href="/tags/PythonScrapy/" style="font-size: 15px;">PythonScrapy</a> <a href="/tags/Python语法/" style="font-size: 15px;">Python语法</a> <a href="/tags/Python爬虫/" style="font-size: 15px;">Python爬虫</a> <a href="/tags/递归/" style="font-size: 15px;">递归</a> <a href="/tags/ssh/" style="font-size: 15px;">ssh</a> <a href="/tags/Strace/" style="font-size: 15px;">Strace</a> <a href="/tags/Tcpdump/" style="font-size: 15px;">Tcpdump</a> <a href="/tags/Zabbix/" style="font-size: 15px;">Zabbix</a> <a href="/tags/SVN/" style="font-size: 15px;">SVN</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/12/15/Python-Scrapy入门/">Python_Scrapy入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/15/Python-Scrapy命令行工具/">Python_Scrapy命令行工具</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/15/Python-网络爬虫初识/">Python_网络爬虫初识</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/15/Python-网络爬虫-cookies操作/">Python_网络爬虫_cookies操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/15/Python-模拟登陆新浪微博/">Python_模拟登录新浪微博</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/15/Python-request模块/">Python_request模块</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/15/Python-random模块/">Python_random模块</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/15/Python-psutil模块/">Python_psutil模块</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/15/Python-IPy模块/">Python_IPy模块</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/15/Python-getpass模块/">Python_getpass模块</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://luxinle.github.io/" title="卢会军的技术博客" target="_blank">卢会军的技术博客</a><ul></ul><a href="http://qinyueming.cc/" title="秦悦明的技术博客" target="_blank">秦悦明的技术博客</a><ul></ul><a href="http://www.jianshu.com/users/a4a7b9d0c23e/latest_articles" title="我的简书" target="_blank">我的简书</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">SateZheng.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>